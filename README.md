# MapReduce using Hadoop, Apache Spark, and Python
This application is designed to analyze tweets for the presence of racist language. The primary goal is to identify and quantify tweets containing specific racist keywords over different years. This application leverages Hadoop for distributed storage, Apache Spark for processing, and Python for scripting and automation.

### Dataset
You can get sample data in the [reusable-data/](reusable-data) directory file, or if you want to modify the data yourself, you can use the source code in the ```collecting_data/``` directory file.

### Requirements
1. Python ^3.8.x
2. Hadoop (NameNode, DataNode, YARN)
3. Apache Spark (PySpark)
4. pandas
5. Jupyter Notebook

## Installation
I have explained the steps for installing Hadoop in the [hadoop_spark/](hadoop_spark/README.md) file, you can immediately read and follow each step.

## Usage
